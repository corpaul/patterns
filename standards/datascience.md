<a href="https://github.com/researchart/patterns/issues"><img  align=left 
src="https://p19cdn4static.sharpschool.com/UserFiles/Servers/Server_269983/Image/Feedback2.jpg"></a>

[about](https://arxiv.org/pdf/2010.03525.pdf) ::
[comment](https://github.com/researchart/patterns/issues) ::
[revise](https://github.com/researchart/patterns/edit/master/artifact.md) ::
[authors](#authors) 

-----------

Feedback on the following proposed standard is requested, before Feb 15, 2021.

<br clear=all>

# Exploratory Data Science in SE  

<em>Repository Mining, and other data-centric analysis methods. 
The topic includes studies that analyze existing software engineering artifacts via data exploration.</em>

##Application

This standard applies to studies that primarily analyze existing **software engineering artifacts**, defined as ``tangible by-products produced during the development of software'' [6]. That analysis may be automatic (e.g. via data mining)  or manual (e.g. via subject matter expert interview) or some combination of both. The artifacts analyzed may  includes ( but is not limited to): source code, source code changes (commits, pull requests, reviews), requirements documents, design diagrams, communications between stakeholders (e.g., issue reports, emails, chat, app reviews), documentation (e.g. system documentation, internet forums, questions and answer sites, written and video tutorials), software ecosystems (packages, libraries, dependencies, and distributions), system logs (execution, continuous integration), interaction data (software usage data, developer interactions), code vulnerabilities, UML models, etc. The output of such data science are many, including (but not limited to)  results about particular hypothesis; data sets (to be used in subsequent challenge problems or research directions); some insight presented as a position paper (e.g. that proposes future work for groups of researchers); the tools used for the analysis (e.g. some software analytics toolkit).

Note that:

- If the analysis focused on the toolkit, rather that some new conclusions generated by the toolkit, consider the **Artifacts Standard** 
- If the analysis focuses on a single, context-rich setting (e.g., a detailed analysis of a single repository), consider the **Case Study Standard**.
- If the analysis selects a subset of available data, consult the **Sampling Supplement**.
- If the analysis is more qualitative in nature, focusing on a smaller set of artifacts to deeply analyze their contends (e.g., qualitative analysis of code review comments), consider the *Discourse Analysis Standard**.
- If data visualizations are used, consider the **Information Visualization Supplement**. Note that repository data may be numerous, which visualizations should take into account to stay legible.
- If the temporal dimension is analyzed, consider the **Longitudinal Studies Standard**. 
- If the study is an  intervention, consider Action Research/Experiment.

## Specific Attributes

Data exploration  in SE is still a rapidly evolving field. Hence, the following criteria are approximate and many exceptions exist to these criteria. Reviewers should reward sound and novel work and, where possible, support a diverse range of studies.


### Essential

- Clearly discusses methods; e.g., how and why the data was selected, pre-processed, filtered, and categorized. E.g. what (and why) prior work was selected as for the purposes of baseline comparisons.  If a data preprocessor changes test data, then - - Clearly make the transformation explicit and justify why that particular pre-process was applied.
- Clearly discusses   motivation:  (e.g. why is it useful/timely to explore this  problem and/or analysis method?).
- Clearly discusses data: described the source of the data as well as statistics (descriptive or otherwise) on that data. This data section should clearly describe how and why the data was selected, pre-processed, filtered, and categorized from the source highlighted above. Automated or manual heuristics used in this process must be documented.
- Clearly describes   related work, results.   
- Ensures that the paper takes appropriate measures against false discoveries due to multiple hypothesis testing.
- Clearly discusses threats to validity using an appropriate framework (either using common standards [7] or a threat to validity appropriate to this study). Clearly describes important data issues that may affect the findings, and EITHER motivates how they are mitigated OR evaluates the impact of the issue (e.g., by a subsequent analysis) OR clearly documents the limitations while avoiding overclaiming.

### Desirable

- Has evaluated the paper against the AAAI Reproducibility checklist [9].
- Data is processed by multiple learners, of different types, e.g. regression, bayes classifier, decision tree, random forests, SVM (maybe with different kernels); e.g. see [0] for guidance.
- Data is processed multiple times with different randomly selected training/test examples; results of which are compared via significance tests and effect size tests.
- Study carefully selects the hyperparameters that control the data miners (e.g. via are a careful analysis of settings seen in related work; e.g. via some automatic hyperparameter optimizer).
- Compares against baselines; i.e. reproduces and/or replicates  prior work related work (perhaps with some small improvements or even a “negative” report commenting that it was not possible to achieve reproduction or replication).
- For studies not based on proprietary  data:  a replication package is made available that conforms to SIGSOFT standards for a functional artifact. if  data cannot be shared (e.g. an  industrial case study), it is desirable to create a sample dataset that can be shared to illustrate the use of the algorithms.
- Data sanity checks: some non-trivial portion of the data was selected and manually inspected. 

### Extraordinary

- Leverages temporal data via longitudinal analyses when appropriate.
- Triangulates with qualitative data analysis of selected samples of the data. 
- Triangulates with other data sources, such as surveys or interviews.
- Reports findings to, or interacts with, authors of SE artifacts to double check with them.

## Examples of Acceptable Deviations

- Using lighter and less precise data processing (e.g. keyword matching or random subsampling) if the scale of data is too large for a precise analysis to be practical.
- Industry-based studies that cannot share their data do not provide a replication package.
- Data not shared since it is impractical to share (too large, too sensitive).
- Not using temporal analysis techniques such as time series when the data is not easily converted to time series (e.g. some aspects of source code evolution may not be easily modelled as time series).
- Not all studies need statistics and hypotheses. Some studies can be purely or principally descriptive.
  - Different exproationations have different requirements [8]:
    - For example, summarizing past data might only need some topic modeling regression on past data since the goal of that study is not to predict on figure cases). 
    - But There are other kinds of studies that need extensive evaluation via “hold out sets” (where the available data is divided into multiple train and test sets) since the goal of those studies is to make predictions on as-yet-unseen data.

## Antipatterns

- Use of statistical tests that assume normal distributions (without first checking for normality).
- If using Bayesian statistics, not motivating the priors used in the study. 
- Claims  causation unless an actual intervention took place; (e.g., installing a bot to monitor a repository), or the methodology is adequate to do so.
- Pre-processing changes training and test data; e.g. while it may be useful to adjust training data class distributions via (say) sub-sampling of majority classes, that adjustment should not applied to the test data (since it is important to assess the learner on the kinds of data that might be seen “in the wild”).
- Unethical data collection or analysis (e.g. harvest personal information unnecessarily).
- Significant tests without effect size tests.
- Reporting a median, without any indication of variance (e.g., a boxplot).
- Multiple trials conducted, but no disclosure or discussion on the variation between trials. 

## Invalid Criticisms 

- Data is not appropriate for the study (e.g., using bug reports comments as code review data).
- Does not have a reproduction package. Currently, only 60% of SE papers from FSE,ASE, EMSE etc come with reproduction packages. Hence we say such packages are desirable, but not essential, since the community does not judge them as essential.
- Findings are not actionable: not all studies may have directly actionable findings in the short term.
- "Needs more data" as a generic criticism without a clear, justified reason.
- Study does not use qualitative data.
- Study does not make causal claims, when it cannot.
- Study does not use the most precise data source, unless the data source is clearly problematic for the study at hand. Some data is impractical to collect at scale.
- Study does not use method XYZ. Apply this criticism with care. Data science is a very broad field and no paper can explore all parts.
- Study does not analyze data ABC. Apply this criticism with care. Data science is a very broad field and no paper can explore all parts.

## Suggested Readings

[1]: Hemmati, Hadi, et al. "The msr cookbook: Mining a decade of research." 2013 10th Working Conference on Mining Software Repositories (MSR). IEEE, 2013.
[2]: Robles, Gregorio, and Jesus M. Gonzalez-Barahona. "Developer identification methods for integrated data from various sources." (2005).
[3]: Dey, Tapajit, et al. "Detecting and Characterizing Bots that Commit Code." arXiv preprint arXiv:2003.03172 (2020).
[4]: Hora, Andre, et al. "Assessing the threat of untracked changes in software evolution." Proceedings of the 40th International Conference on Software Engineering. 2018.
[5]: Herzig, Kim, and Andreas Zeller. "The impact of tangled code changes." 2013 10th Working Conference on Mining Software Repositories (MSR). IEEE, 2013.
[6]: Berti-Équille, L. (2007). Measuring and Modelling Data Quality for Quality-Awareness in Data Mining.. In F. Guillet & H. J. Hamilton (ed.), Quality Measures in Data Mining , Vol. 43 (pp. 101-126) . Springer . ISBN: 978-3-540-44911-9.
[7]: Wohlin, C., Runeson, P., Höst, M., Ohlsson, M. C.,, Regnell, B. (2012). Experimentation in Software Engineering.. Springer. ISBN: 978-3-642-29043-5
Wohlin’ standard thrrs
[8]  Raymond P. L. Buse and Thomas Zimmermann. 2012. Information needs for software development analytics. In Proceedings of the 34th International Conference on Software Engineering (ICSE '12). IEEE Press, 987–996.
[9]  https://aaai.org/Conferences/AAAI-21/reproducibility-checklist/






 

=======
Todo
XXXX small
XXXX large

XXXX data collection is controlled
XXXX data mining is on a data set that was no designed for the goal of the current study


Feature engineering intensive
Feature engineering lite 

Predictive studies (what might be)
vs descriptive studies (what has been or what is)

===========

# Data Science 

Repository Mining, and other data-centric analysis method. Includes studies that analyze existing software engineering artifacts via data exploration

## Application   

This standard applies to studies that primarily analyze existing **software engineering artifacts**, defined as ``tangible by-products produced during the development of software'' [6]. That analysis may be automatic (e.g. via data mining)  or manual (e.g. via subject matter expert interview) or some combination of both. The artifacts analyzed may  includes ( but is not limited to): source code, source code changes (commits, pull requests, reviews), requirements documents, design diagrams, communications between stakeholders (e.g., issue reports, emails, chat, app reviews), documentation (e.g. system documentation, internet forums, questions and answer sites, written and video tutorials), software ecosystems (packages, libraries, dependencies, and distributions), system logs (execution, continuous integration), interaction data (software usage data, developer interactions), code vulnerabilities, UML models, etc. The output of such data science are many, including (but not limited to)  
results about particular hypothesis, 
data sets (to be used in subsequent challenge problems or research directions)  
Some insight presented as a position paper (e.g. that proposes future work for groups of researchers)
The tools used for the analysis (e.g. some software analytics toolkit).


## Specific Attributes

We stress that the use of data exploration  in SE is still a rapidly evolving field. Hence, the following criteria are approximate and many exceptions exist to these criteria. Reviewers should reward sound and novel work and, where possible, support a diverse range of studies.


| Importance    | Attribute |
| ----------    | -----------|
| Essential     |   The only essential property of a position papers are  brevity (it should not be long).   As for the other kinds of paper, see below. |
|                     | Clear motivations (e.g. why use this data rather than some other; e.g. why use these learners rather than some other; e..g. Why explore this problem rather
|                     |        that some other.|
|                     |  Clearly describes how and why the data was selected, pre-processed, filtered, and categorized.  
|                    |          Automated or manual heuristics used in this process should be documented      
|               | If pre-processort changes test data, then clearly make the transformation explicit and justify why that particular pre-process was applied. |
|               | Clearly describes important data issues that may affect the findings, and EITHER motivates how they are mitigated OR evaluates the impact
                   of the issue (e.g., by a subsequent analysis) OR clearly documents the limitations and avoids overclaiming |
|               | If using frequentist statistics, takes appropriate measures against false discoveries due to multiple hypothesis testing |
|               | Checks data distribution before making statistical tests (Repository data is often not normally distributed) |
|               | If using bayesian statistics, clearly motivates the priors taken | 
|               | Does not claim causation, unless an actual intervention took place (e.g., installing a bot to monitor a repository), or the methodology is adequate to do so |
|               | Avoids inadvertently implying causation by clearly highlighting speculations in discussion sections, or strictly talking about associations between variables |
|               | Discusses threats to validity using an appropriate framework (either using common standards [7] or a a threats to validity appropriate to this study)|
 to validity section more appropriate to the current If observing an association between variables, document possible confounding factors | 
|               | Uses correct analyses [XXX reference]  for temporal data (e.g. survival analysis if the data is censored, time series analysis) |
|               | Has enough information to be reproduced at a high level |




| Desirable     |  It is highly desirable that postion papers have a visionary quality. As for the other kinds of papers, see below.|
|                    | Novelty is highly desirable. Where possible, authors should explore a new problem type (or a new area within an existing problem space) which has never been studied in the literature. 
|                     | Reproductions and replications of prior work (perhaps with some small improvements) should be encouraged and rewarded. 
|                     | For studies not based on proprietary  data: To enable open science,  a replication package should be made available that conforms to SIGSOFT standards for a functional artifact (if  data cannot be shared (e.g., industrial case study), it is desirable to create a sample dataset that can be shared to illustrate the use of the algorithms)
 |                    |If using frequentist statistics, emphasizes practical significance (effect sizes) over statistical significance 
                         (with large sample sizes, virtually any difference can be significant) | 
|                     | Provides evidence that a convincing sample of the data was manually inspected as a sanity check |
|               | SOme random sample of the data is manually inspected (to check data is “sane”|
|               | Results in the current study are baseline against some prior state-of--the-art result |
|               |       And the method for selecting that prior state of the art is explained|
|              | Data is processed by multiple learners, of different types, e.g. regression, bayes classifier, decision tree, random forests, SVM (maybe with different kernels); e.g. see [0] for guidance||
|               |If the study relies on some key heuristic, estimates the imprecisions of said heuristics in a subsequent analysis 
                   (e.g., manually investigating a sample of automatically classified artifacts to estimate the frequency of misclassifications) |
|               | Claims limited forms of causality when appropriate (e.g. using lag variables and precedence when analyzing time series) |
|               | Leverages temporal data via longitudinal analyses when appropriate |
|               | Provides a replication package that includes scripts for data analysis and pre-processing, and
                  EITHER the data that was collected itself(preferred, as it has less risks of disappearing or being out of date) OR
                   links to the data (e.g. urls of repository with commit hashes specifyingspecifing exact versions)|
|               | Clearly formulates research hypotheses, rather than adopting a purely descriptive approach  |

| Extraordinary |             |  Triangulates with qualitative data analysis of selected samples of the data |
|               | Triangulates with other data sources, such as surveys or interviews |
|               | Reports findings to, or interacts with, authors of SE artifacts to double check with them |

## General Quality Criteria

**A key concern with SE artifacts is that they are a by-product of software development; they are not purposefully collected for researchers to study**. As such, they are very often subject to data quality issues. To give a few examples: developers may be identified by accounts or emails, but they may use several of these [2]; some accounts may not be real users, but bots [3]; files or source code entities may change their names over time [4]; source code changes may be tangled [5]. Many other issues are documented in the literature (see the ``MSR Cookbook''[1] in the selected readings for a partial list). SE artifacts can also be complex and subject to varying degrees of analysis.

Quantitative studies of SE artifacts are often evaluated in terms of construct validity, external validity, reliability, reproducibility, and possibly internal validity.

## Antipatterns

Does not have a reproduction package. Currently, only 60% of SE papers from FSE,ASE, EMSE etc come with reproduction packages. Hence we say such packages are desirable, but not essential, since the community does not judge them as essential.
- Does not check data quality, does not filter data at all, or fails to document quality issues
- Does not conduct sanity checks (e.g. a manual inspection of some small X% or a randomly selected subset of the data)
- Unwarranted claims of causation, and other statistical issues
- Harvest personal information unnecessarily
- Data is not appropriate for the study (e.g., using bug reports comments as code review data)
- Quality of data is too compromised for the study (e.g. a study of an ecosystem with too much missing data)
- wrong data distribution: not varied enough for the claims (**Mei's paper**), or does not match the level of abstraction needed (e.g. too coarse-grained or too fine-grained, ecological fallacies)
 

- Significance tests (e.g., Mann-Whitney Wilcoxan test) without effect size tests. 
   - "Significance" tests if distriubtuins can be distingused fro each other.
   - But "effect size" tests are required to check if the difference between distributions is "interesting" and not just a trivirally "small effect".
- Multiple trials conducted, but no disclosure or discussion on the variation between trials. Reporting a median, without any indication of variance (e.g., a boxplot), does not indicate potential variation between each trial.

 


<!-- some gray area here ... -->

## Invalid Criticisms

- Study does not use qualitative data
- Study does not make causal claims, when it cannot
- Study does not use the most precise data source, unless the data source is clearly problematic for the study at hand. Some data is impractical to collect at scale.
- Study does not apply to settings wherewere it is impractical to collect data (as long as the settings are "close enough"?)
- Findings are not actionable: not all studies may have directly actionable findings in the short term
- "Needs more data" as a generic criticism without a clear, justified reason 
- "Effect size too small" as a generic criticism without justification
- Applicability to OSS/Industrial code only, not checked on the other one 

- That a reproduction or replication study is just a repeat of prior work (see **Replication Supplement**). Replications and reproductions are essential to the scientific method and the advancement of a field. 
  - To the authors writing such  reprodictiom papers: (a) motivate the importance of the initial work (being replicated or reproduced here) ; (b) offer new insight beyond the initial study. 
- That an approach is not benchmarked against an inappropriate or unavailable baseline. If a state-of-the-art approach lacks an available and functional implementation, it is not reasonable to expect the author to recreate that approach for benchmarking purposes. 
- That results are negative (i.e., worse than a random search or other state-of-the-art). Such results are still valuable and advance the state of the field.

- Study has been done already, or too similar to an existing study 

## Examples of Acceptable Deviations

- Studies of industrial data that can not be shared
- Data that is impractical to share (too large, too sensitive)
- Using lighter and less precise data processing (e.g. keyword matching) if the scale of data is too large for a precise analysis to be practical
- Not using temporal analysis techniques such as time series when the data is not easily converted to time series (e.g. some aspects of source code evolution may not be easily modelled as time series)
- Not all studies need statistics and hypotheses. Some studies can be purely or principally descriptive.

## Notes

This standard focuses on quantitative analysis of Software Engineering (SE)artifacts. However, as SE artifacts are used in many different ways, this standard can, and should, be used jointly with other standards and supplements:

- If the analysis focused on the toolkit, rather that some new conclusions generated by the toolkit, consider the **Artifacts Standard**
- If the analysis focuses on a single, context-rich setting (e.g., a detailed analysis of a single repository), consider the **Case Study Standard**.
- If the analysis selects a subset of available data, consult the **Sampling Supplement**.
- If the analysis is more qualitative in nature, focusing on a smaller set of artifacts to deeply analyze their contends (e.g., qualitative analysis of code review comments), consider the **Discourse Analysis Standard**.
- Consider sharing data according to the **Open Science Standard**, if possible.
- If data visualizations are used, consider the **Information Visualization Supplement**. Note that repository data may be numerous, which visualizations should take into account to stay legible.
- If the temporal dimension is analyzed, consider the **Longitudinal Studies Standard**. 
- If a predictive approach leveraging repository data  is evaluated (eg. a recommender systems, a machine learning algorithm), consider the **Engineering Research Standard** and (possibly) the **Machine Learning Supplement**
- If an intervention, consider Action Research/Experiment.

Different exproationations have different requirements [8]:.
For example, summarizing past data might only need some topic modeling regression on past data since the goal of that study is not to predict on figure cases). 
But There are other kinds of studies that need extensive evaluation via “hold out sets” (where the available data is divided into multiple train and test sets) since the goal of those studies is to make predictions on as-yet-unseen data.
Might not need a test set.

Position  statements (also known as a NEIR paper): the history of our field is that once a research groups specifies a goal and offers some initial data, then very large communities forma round those initial results (e.g. defect prediction, test case prioritization, spectral diagnosis, crossc-omnapy learning, effort estimation, etc). Therefore, before 100s of researchers write 1000s of papers on some area, we think there is a role for “look before you leap”; i.e. challenge statements that might serve to guide much further work. Exa,pels; Hersleb;s  ICSE’14 keynote, Haye’s 1979 Naive Physics Manifesto; the DUO paper MSR’18; 
Summarization studies: describing patterns that already exist ( not data mining, may just use visualizations, simple correlation, etc).
Abductive Exploratory data science: from facts, derive new questions for next round of data collection. Eg. active learning  reading large numbers of documents with a SVM looking on trying to learn your model of “that’s interesting”
Deductive Confirmatory  data science: from facts and rules, derive an expectation which we want to check.  Note to reader: for an interesting variant of CEDS, see the registered reports standard).
Inductive Generative  data science: from data, generate some model (either an unsupervised model (e.g. cluster,s association rules) or supervised model (a deep learning network, decision trees,etc))

**Causal relationships.** Establishing causal relationships based on SE artifacts collected after the fact is very difficult, due to the many possible confounding factors involved that are out of our control. Possible ways forward include:

- Formulate hypotheses based on SE artifact data and validate these hypotheses with other analyses or in follow-up studies with other methodologies
- Discard or account for the influence of the most important confounding factors by additional analyses
- Leverage appropriate statistical analyses on temporal data for limited causality claims, such as using time series analysis with lag variables to claim precedence. Note that very complex artifacts may not be easily modelled as time series (e.g. structured source code).
- Have an actual intervention that can be subsequently observed on SE artifacts

**Modelling versus predicting.** Prefer a data modelling approach (e.g., a regression model) when your goal is to detect relationships between variables or to explain a phenomenonphenomenoms; prefer a predictive approach (e.g., a machine learning algorithm) when absolute predictive accuracy is preferred to model interpretability. **this still needs some work and possibly integration above**

## Suggested Readings


-[0]:  Baljinder Ghotra, Shane McIntosh, and Ahmed E. Hassan. 2015. Revisiting the impact of classification techniques on the performance of defect prediction models. In Proceedings of the 37th International Conference on Software Engineering - Volume 1 (ICSE '15). IEEE Press, 789–800.
- [1]: Hemmati, Hadi, et al. "The msr cookbook: Mining a decade of research." 2013 10th Working Conference on Mining Software Repositories (MSR). IEEE, 2013.
- [2]: Robles, Gregorio, and Jesus M. Gonzalez-Barahona. "Developer identification methods for integrated data from various sources." (2005).
- [3]: Dey, Tapajit, et al. "Detecting and Characterizing Bots that Commit Code." arXiv preprint arXiv:2003.03172 (2020).
- [4]: Hora, Andre, et al. "Assessing the threat of untracked changes in software evolution." Proceedings of the 40th International Conference on Software Engineering. 2018.
- [5]: Herzig, Kim, and Andreas Zeller. "The impact of tangled code changes." 2013 10th Working Conference on Mining Software Repositories (MSR). IEEE, 2013.
- [6]: Ref TBD
- [7]: Wohlin, C., Runeson, P., Höst, M., Ohlsson, M. C.,, Regnell, B. (2012). Experimentation in Software Engineering.. Springer. ISBN: 978-3-642-29043-5
Wohlin’ standard thrrs
[8]   Raymond P. L. Buse and Thomas Zimmermann. 2012. Information needs for software development analytics. In Proceedings of the 34th International Conference on Software Engineering (ICSE '12). IEEE Press, 987–996.

## Exemplars

- TBD
- Promises and perils of mining github paper has good sanity checks. 


Francisco Gomes de Oliveira Neto, Richard Torkar, Robert Feldt, Lucas Gren, Carlo A. Furia, Ziwei Huang: Evolution of statistical analysis in empirical software engineering research: Current state and steps forward. J. Syst. Softw. 156: 246-267 (2019)

Neto, A. A. & Conte, T. (2013). A conceptual model to address threats to validity in controlled experiments.. In F. Q. B. da Silva, N. J. Juzgado & G. H. Travassos (eds.), EASE (p./pp. 82-85), : ACM. ISBN: 978-1-4503-1848-8





